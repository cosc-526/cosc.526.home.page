{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **`M.0.Overview of codebook, answers, instructions, and libraries`**"
      ],
      "metadata": {
        "id": "el9sjRdm9gjv"
      },
      "id": "el9sjRdm9gjv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`I. how.To use course codebook`**  \n",
        "1. Notebook works in both Jupyter Notebooks & Colab.  \n",
        "2. Google built [colab](https://colab.research.google.com/) on top of Notebooks to add more features.  \n",
        "3. Cosc.526 was configured in Colab with expand/collapse sectioning.  \n",
        "4. There are 9 parent sections for each course module.  \n",
        "5. Modules contain either exercise and assignment or both.  \n",
        "6. **ALL** additional links and resources are to quality resources and tools.  \n",
        "=> Consider bookmarking them for future analysis activities!  \n",
        "7. Your going to be \"coding.\" Consider flipping a monitor vertically to help code.    \n",
        "8. A minimum of two monitors is recommended.   \n",
        "9. Try open 2 workbooks at same time on different monitors!\n",
        "10. **p.s.** colab offers screen companions like sparks, corgis, and crabs\n",
        "\n",
        "## **`II. What qualifies as a task answer?`**  \n",
        "- `Answer:` answers are outcomes generated from code that executes.\n",
        "- when you generate a .pdf, simply dont delete answer beneath ur code run.\n",
        "- ***What if your task's code isn't running?***  \n",
        "- Add one textbox beneath block titled ***Enter solution here***  \n",
        "- Add title: \"Professor: explanation of current state\"\n",
        "- Succinctly explain work done (for grading points).  \n",
        "\n",
        "## **`III. Umbrella instructions`**  \n",
        "1. ### **`How do I solve exercise and assignment tasks?`**  \n",
        "a. by writing code.  \n",
        "b. by modifying code provided.  \n",
        "c. by answering specified questions concisely.  \n",
        "d. as necessary, reference any resource that helped accomplish work.\n",
        "\n",
        "2. ### **`What are the submission requirements?`**  \n",
        "d. `Answer:` upload a **.pdf** copy after renaming it to...    \n",
        "e. => weeks 1,2: last.first.exercise.and.assignment.M.#.pdf  \n",
        "f. => weeks 3-9: last.first.assignment.M.#.pdf  \n",
        "\n",
        "3. ### **`How do I generate a .pdf to submit work?`**\n",
        "g. `Answer:` Jupyter Notebook has File\\Save As\\ adobe.pdf  \n",
        "h. **What do you do if .pdf generation fails?**  \n",
        "i. task.1) when in doubt, always submit a **.ipynb** file!  \n",
        "j. task.2) inform professor requesting permission to submit **.ipynb**   \n",
        "k. task.3) research and engineer the fix; Youtube and [Jupyter.forum](https://discourse.jupyter.org/).\n",
        "\n",
        "4. ### **`What is an Expected Outcome section in tasks below?`**  \n",
        "l. `Answer:` Its a solution to a provided task to help guide your efforts.   \n",
        "m. They're regularly provided, but only sometimes.   \n",
        "n. They're located directly beneath codeblock=> **Enter solution here**  \n",
        "\n",
        "5. ### **`How many code blocks should my output answer be?`**    \n",
        "o. `Answer` as few as possible which means ideally no more than \"1\"  \n",
        "=> Plan and organize final outcomes. Brevity is key!  \n",
        "=> points will be \"detracted\" for unorganized and unprofessional codeblocks  \n",
        "\n",
        "## **`IV. cosc.526 Python required libraries`**  \n",
        "1. Library engineering assistance is next to help get you running now.  \n",
        "2. Confirm library code executes completly; `V. Library Resources`  \n",
        "3. If not, research and solve right away! [Jupyter.forum](https://discourse.jupyter.org/)  \n",
        "4. **`Why?`** Errors occur for many reasons and can be challenging to fix.    \n",
        "3. Use source library document pages for additional assistance and [pypi.org](https://pypi.org/)   "
      ],
      "metadata": {
        "id": "3RMqlKER-Hv1"
      },
      "id": "3RMqlKER-Hv1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`V. Library Resources`**     \n",
        "1. Python coding companion => [Python reference.library.Cosc.526.pdf](https://github.com/cosc-526/cosc.526.home.page/blob/main/reference.library.COSC.526.pdf) <=`print.ME`\n",
        "3. Use [Python Docs;](https://docs.python.org/3/) **`they're simply the best.`** [TinaTurner](https://www.youtube.com/watch?v=GC5E8ie2pdM) we love you."
      ],
      "metadata": {
        "id": "R34rZNc2oE7T"
      },
      "id": "R34rZNc2oE7T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `They're many reasons for library initialization errors`\n",
        "- #### research and fix using quality key word searchers like\n",
        "- #### `git`, `docs`, `colab`, `error`, paste error into search  \n",
        "- #### recommendation: avoid \"news\" type websites\n",
        "- #### recommendation: google provides optimized data science outcomes  "
      ],
      "metadata": {
        "id": "5yljl9o-_JkC"
      },
      "id": "5yljl9o-_JkC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`numpy as np`**  badass scientific for multi-dimensional arrays and math functions.\n",
        "- **`pandas as pd`**  spreadsheet like data manipulation and analysis.\n",
        "- **`os`** talk to your op system and nav directories, files, and sys commands.\n",
        "- **`matplotlib.pyplot as plt`** for plotting and visualization.\n",
        "- **`pyspark`** API for Apache Spark for processing large-scale datasets in parallel.\n",
        "- **`keras`** neural networks API on top TensorFlow for deep learning w images and NLP."
      ],
      "metadata": {
        "id": "yuSoFuh-rWoj"
      },
      "id": "yuSoFuh-rWoj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Engineered install convience window :>)`"
      ],
      "metadata": {
        "id": "6OcCyQtLo9XL"
      },
      "id": "6OcCyQtLo9XL"
    },
    {
      "cell_type": "code",
      "source": [
        "#=> why provided? \n",
        "#=> you so you can focus on reading all LMS links and resources\n",
        "\n",
        "# Are needed libraries installed?\n",
        "# => on.jupyter:  !pip show  <library.name> \n",
        "# => on.terminal:  pip show scikit-learn\n",
        "\n",
        "# how.To Install library\n",
        "# => on.jupyter:  !pip install  <library.name> \n",
        "# => on.terminal:  pip install scikit-learn\n",
        "# => package manager => https://pypi.org\n",
        "\n",
        "#=> New to data mining? install all!\n",
        "#!pip install numpy\n",
        "#!pip install pandas\n",
        "#!pip install pyspark\n",
        "#!pip isntall tensorflow\n",
        "#!pip install keras\n",
        "#!pip install matplotlib\n",
        "#!pip install scikit-learn\n",
        "\n",
        "#=> Example of quality sources\n",
        "#https://scikit-learn.org/stable/install.html\n",
        "#https://spark.apache.org/docs/latest/api/python/getting_started/install.html\n",
        "#https://dvgodoy.github.io/handyspark/includeme.html#:~:text=HandySpark%20is%20a%20package%20designed,returning%20pandas%20objects%20straight%20away.\n",
        "\n",
        "#=> how.To update all installed libraries ?\n",
        "#thanks! rbergeron@snhu.edu\n",
        "#python3 -m pip list --outdated --format=json | jq -r '.[] | \"\\(.name)==\\(.latest_version)\"' | xargs -n1 pip3 install -U\n",
        "\n",
        "#misc\n",
        "# python -m pip freeze          # what are all installed packages in virtualenv"
      ],
      "metadata": {
        "id": "1wR997SH3w_d"
      },
      "id": "1wR997SH3w_d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly                #SVM hyperplane graphing\n",
        "import plotly.express as px  #SVM hyperplane graphing\n",
        "\n",
        "#unsupervised\n",
        "import sklearn\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler #kind of important\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#supervised\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "print(\"library.wheels.up\")"
      ],
      "metadata": {
        "id": "VBeXo-I2X-j_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53507119-402f-4aee-d9da-ca1c98516e75"
      },
      "id": "VBeXo-I2X-j_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "library.wheels.up\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark: [Interstellar: we're going to spark it](https://www.google.com/search?sxsrf=APwXEdfIje02_Nln52w2MKqzRUz6KA3sBA:1685806695619&q=interstellar+were+going+to+spark+it&tbm=isch&sa=X&ved=2ahUKEwjG_4Gqt6f_AhUMEFkFHaG8BfcQ0pQJegQINxAB&biw=1200&bih=2010&dpr=0.9#imgrc=M4cdnQpK79fKhM)"
      ],
      "metadata": {
        "id": "7ZFwo-53495W"
      },
      "id": "7ZFwo-53495W"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "#!pip show pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz0lPt0nIw4H",
        "outputId": "a0a70152-4cbc-4e24-8dc3-010b41d82040"
      },
      "id": "dz0lPt0nIw4H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=0bef0714444be32bc2dc7ad240e91d89922e9da7bc1243b6c611b556fdd0d914\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=> Option A\n",
        "import tensorflow as tf\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\").getOrCreate() #create spark session\n",
        "#\"local\" argument passed to master; run spark in local mode on Colab runtime.\n",
        "\n",
        "from pyspark.sql import SparkSession #note: if run notebooks, need Java\n",
        "import pyspark.sql.functions as F\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import LogisticRegressionTrainingSummary\n",
        "from pyspark.ml.classification import LogisticRegressionModel, LogisticRegressionSummary\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.classification import NaiveBayes  #Perceptron, \n",
        "from pyspark.ml.classification import DecisionTreeClassifier, LinearSVC\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "print(\"spark.library.wheels.up\")\n",
        "\n",
        "#=> when done with spark goto end of assignment & close spark; run \n",
        "#spark.stop()  <optional:  run sc.stop()>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4qW1snx9H7U",
        "outputId": "ff72b89e-9465-456e-dc85-fa489c6b3691"
      },
      "id": "q4qW1snx9H7U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark.library.wheels.up\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `=> Pyspark discrete setting option for Jupyter Notebook, not Colab`  "
      ],
      "metadata": {
        "id": "KWMpLbplJkc8"
      },
      "id": "KWMpLbplJkc8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- import handyspark: additional functionalities on top of PySpark like (?)  \n",
        "- import findspark: locates the Spark install sets environment variables.  \n",
        "- findspark.init(): ibid  \n",
        "- from pyspark import SparkContext, SparkConf  \n",
        "=> pyspark classes create a SparkContext object; Colab does automatically  \n",
        "`note:` Jupyter Notebook requires `Java;` Colab prebuilt with\n",
        "```\n",
        "#=> Option B: Notebook discrete performance settings\n",
        "import handyspark, findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "sc = SparkContext(master=\"local[4]\")        # .o.n.l.y. run ONCE\n",
        ">>> sc => example how to create a SparkContext with a local master and 4 cores. \n",
        "```"
      ],
      "metadata": {
        "id": "-9o5WKR1N32v"
      },
      "id": "-9o5WKR1N32v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`M.1.Introduction to data mining`**\n"
      ],
      "metadata": {
        "id": "khVZ7DoGDiBn"
      },
      "id": "khVZ7DoGDiBn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`exercise.M.1`** - Python Warmup"
      ],
      "metadata": {
        "id": "X-de8q4f_q13"
      },
      "id": "X-de8q4f_q13"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`Overview and Directions`**"
      ],
      "metadata": {
        "id": "cW6PQ4KAFhfd"
      },
      "id": "cW6PQ4KAFhfd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Practice importing and parsing information. \n",
        "* Focus on learning and solving versus coding perfectly.  \n",
        "* Perform tasks without assistance from clever sources.  \n",
        "\n",
        "#### **`Desired outcomes`**  \n",
        "- Experience open, read, and writing of external delimited files.  \n",
        "- Navigate basic text mining preprocessing like white space stripping.  \n",
        "- Refresh expereience with iterator, conditionals, and functions.  \n",
        "\n",
        "####**`Additional Resources`**  \n",
        "- course [Python reference.library.Cosc.526.pdf](https://github.com/cosc-526/cosc.526.home.page/blob/main/reference.library.COSC.526.pdf)  \n",
        "=> contains a deep and wide index of essential Python coding.  \n",
        "- Bookmark the masters of text preprocessing  \n",
        "=> [Jurafsky and Martin, Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)  \n",
        "=> additional quality textbook on classical text mining  \n",
        "=> [Wiess,S.,Indurkhya,N.,Zhang,T.,(2015), Fundamentals of predictive text mining, 2nd, Springer.](https://www.amazon.com/Fundamentals-Predictive-Mining-Computer-Science/dp/144716749X/ref=sr_1_1?crid=MUA7UG21IFPD&keywords=Wiess%2CS.%2CIndurkhya%2CN.%2CZhang%2CT.%2C%282015%29%2C+Fundamentals+of+predictive+text+mining%2C+2nd%2C+Springer&qid=1685151591&sprefix=wiess%2Cs.%2Cindurkhya%2Cn.%2Czhang%2Ct.%2C+2015+%2C+fundamentals+of+predictive+text+mining%2C+2nd%2C+springer%2Caps%2C78&sr=8-1)"
      ],
      "metadata": {
        "id": "CWbFlcJXI1Id"
      },
      "id": "CWbFlcJXI1Id"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR2SKZ0V4l36"
      },
      "source": [
        "### **`Task.1`**  - comma-separated values (.csv)"
      ],
      "id": "ZR2SKZ0V4l36"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading and parsing [delimiter-separated values](https://en.wikipedia.org/wiki/Delimiter-separated_values) files like [comma-separated](https://en.wikipedia.org/wiki/Comma-separated_values) and [tab-separated values](https://en.wikipedia.org/wiki/Tab-separated_values) is a regular data science preprocessing activity. It is typically acceptable to request either file format for analysis activities.    \n",
        "- *.csv* files store tabular data like numbers and text in a plain text format. \n",
        "- Plain text may include text, white spaces, carriage returns, transliterals, and other artifacts.    \n",
        "- Each row, or data record, contains a value or nothing, and a comma separates each.    \n",
        "\n",
        "**Tasks**  \n",
        "0. [data.exercise.M.1.csv](https://github.com/cosc-526/cosc.526.home.page/blob/main/data.exercise.M.1.csv) => Nobel prize winners name and age     \n",
        "1. Generate a single value for the total number of rows of data.\n",
        "2. Generate a single value for the total number of columns of data.  \n",
        "3. Calculate the laureates average age as a datatype float.  \n",
        "4. **note:** the solution's answer is structured as a user-defined function (def).  \n",
        "5. A def is not required, it contains extra code to use throughout the course.  \n",
        "6. Recall, whenever feasible, combine answers into a single outcome code block.     \n",
        "\n",
        "**Useful links**  \n",
        "- [Ch.16, Importing Data, Python.Crash.Course, Matthes](https://github.com/cosc-526/cosc.526.home.page/blob/main/textbook.Python.crash.course.matthes.pdf)  \n",
        "[open](https://docs.python.org/3.6/library/functions.html#open), \n",
        "[readlines](https://docs.python.org/3.6/library/codecs.html#codecs.StreamReader.readlines), [rstrip](https://docs.python.org/3.6/library/stdtypes.html#str.rstrip), [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions), [split](https://docs.python.org/3.6/library/stdtypes.html#str.split), [splice](https://docs.python.org/3.6/glossary.html#term-slice), [\"list.love\"](https://docs.python.org/3.6/tutorial/datastructures.html#more-on-lists), [len](https://docs.python.org/3.6/library/functions.html#len), [int](https://docs.python.org/3.6/library/functions.html#int), [format](https://docs.python.org/3.6/library/stdtypes.html#str.format)"
      ],
      "metadata": {
        "id": "cs55wtGeEV7u"
      },
      "id": "cs55wtGeEV7u"
    },
    {
      "cell_type": "code",
      "source": [
        "#=>Enter Your Solution\n"
      ],
      "metadata": {
        "id": "Azp65ID2UV9X"
      },
      "id": "Azp65ID2UV9X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFtuKBBh4l4C"
      },
      "source": [
        "**Task.1 Expected Ouput**\n",
        "```\n",
        "Number of rows of data: 8\n",
        "Number of cols: 3\n",
        "Average Age: 70.875\n",
        "```"
      ],
      "id": "fFtuKBBh4l4C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybWXMZ974l4D"
      },
      "source": [
        "### **`Task.2`** - tab-separated values (.tsv)"
      ],
      "id": "ybWXMZ974l4D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A [tab-separated value (.tsv)](https://en.wikipedia.org/wiki/Tab-separated_values) format is a delimiter-separated value for storing data in a tabular structure like a database table or spreadsheet. Other characteristics include,    \n",
        "- Used to exchange information between databases.\n",
        "- Each record in the table is one line of the text file.\n",
        "- Fields are distinct in a record when separated by the tab character **`\\t`**\n",
        "\n",
        "**Tasks**\n",
        "0. [data.exercise.M.1.tsv](https://github.com/cosc-526/cosc.526.home.page/blob/main/data.exercise.M.1.tsv) \n",
        "1. Repeat Task.1 using the .tsv file.  \n",
        "2. The order and data in columns have changed.  \n",
        "3. If you hardcoded the \"age\" column, research and describe alternative code to alleviate fixed positional indexes.  \n",
        "\n",
        "**Useful links**  \n",
        "- [Python docs - csv file reading and writing](https://docs.python.org/3/library/csv.html#module-csv) \n",
        "- Python handles .tsv files using its \"delimiter\" parameter, \"\\t\"  "
      ],
      "metadata": {
        "id": "XpY0qD0rIsz3"
      },
      "id": "XpY0qD0rIsz3"
    },
    {
      "cell_type": "code",
      "source": [
        "#=>Enter Your Solution\n"
      ],
      "metadata": {
        "id": "0ODxd_9HTCy9"
      },
      "id": "0ODxd_9HTCy9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Ouput:**\n",
        "```\n",
        "Number of rows of data: 11\n",
        "Number of cols: 3\n",
        "Average Age: 62.09090909090909\n",
        "```"
      ],
      "metadata": {
        "id": "_9HmDoDjTNgi"
      },
      "id": "_9HmDoDjTNgi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSr6LhyN4l4E"
      },
      "source": [
        "### **`Task.3`** - Convert diacritics (ä, ö) to ASCII"
      ],
      "id": "LSr6LhyN4l4E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- On your computer, right click an open `data.exercise.M.1.csv` in Notepad.  \n",
        "Observe the Unicode non-English letters in laureates' names like \"Schrödinger.\"\n",
        "- Learn about [Unicode](https://en.wikipedia.org/wiki/Unicode) character standards for representing different types and forms of text.  \n",
        "- Grok that Python 3 [natively supports](https://docs.python.org/3/howto/unicode.html) Unicode, but many tools don't.\n",
        "- Conversion of Unicode to [ASCII](https://en.wikipedia.org/wiki/ASCII) formatting is often necessary in data preprocessing.  \n",
        "\n",
        "**Tasks**\n",
        "0. [data.exercise.M.1.csv](https://github.com/cosc-526/cosc.526.home.page/blob/main/data.exercise.M.1.csv)  \n",
        "1. Read this article on diacritics conversion (e.g., \"ü\" → \"ue\"); [transliteration](https://german.stackexchange.com/questions/4992/conversion-table-for-diacritics-e-g-%C3%BC-%E2%86%92-ue).  \n",
        "2. Analyze and run code block with a dictionary matching Unicode character \"keys\" to their ASCII transliteration \"value.\"\n",
        "=> as a refresher, a dictionary is defined as mydict = { key:value }\n",
        "3. For labeled code sections #3.1 to 3.9, explain succinctly what the code is accomplishing and whether you are or are not familiar with it.  \n",
        "4. Create your inventory mechanism to store this, and more, code blocks (ungraded & no solution).  \n",
        "\n",
        "***More useful links***\n",
        "- [1: replace](https://docs.python.org/3.6/library/stdtypes.html#str.replace), [2: file object methods](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects),  \n"
      ],
      "metadata": {
        "id": "CdUDaILLI7Ro"
      },
      "id": "CdUDaILLI7Ro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17tX_qpT4l4F",
        "outputId": "8cb903f8-3f9f-49b3-c3c9-8abd61942050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Richard Phillips Feynman\n",
            "Shin'ichiro Tomonaga\n",
            "Julian Schwinger\n",
            "Rudolf Ludwig Moessbauer\n",
            "Erwin Schroedinger\n",
            "Paul Dirac\n",
            "Maria Sklodowska-Curie\n",
            "Pierre Curie\n"
          ]
        }
      ],
      "source": [
        "translit_dict = {\n",
        "    \"ä\" : \"ae\",\n",
        "    \"ö\" : \"oe\",\n",
        "    \"ü\" : \"ue\",\n",
        "    \"Ä\" : \"Ae\",\n",
        "    \"Ö\" : \"Oe\",\n",
        "    \"Ü\" : \"Ue\", \n",
        "    \"ł\" : \"l\",\n",
        "    \"ō\" : \"o\",\n",
        "}\n",
        "#3.1\n",
        "with open(\"data.exercise.M.1.csv\", 'r', encoding='utf8') as csvfile:\n",
        "    lines = csvfile.readlines()\n",
        "#3.2\n",
        "# Strip off the newline from the end of each line\n",
        "lines = [line.rstrip() for line in lines]\n",
        "\n",
        "#3.3   \n",
        "# Split each line based on the delimiter (which, in this case, is the comma)\n",
        "split_lines = [line.split(\",\") for line in lines]\n",
        "\n",
        "#3.4\n",
        "# Separate the header from the data\n",
        "header = split_lines[0]\n",
        "data_lines = split_lines[1:]\n",
        "    \n",
        "#3.5    \n",
        "# Find \"name\" within the header\n",
        "name_index = header.index(\"name\")\n",
        "\n",
        "#3.6\n",
        "# Extract the names from the rows\n",
        "unicode_names = [line[name_index] for line in data_lines]\n",
        "\n",
        "#3.7\n",
        "# Iterate over the names\n",
        "translit_names = []\n",
        "for unicode_name in unicode_names:\n",
        "    # Perform the replacements in the translit_dict\n",
        "    # HINT: ref [1]\n",
        "    translit_name = unicode_name\n",
        "    for key, value in translit_dict.items():\n",
        "        translit_name = translit_name.replace(key, value)\n",
        "    translit_names.append(translit_name)\n",
        "\n",
        "#3.8\n",
        "# Write out the names to a file named \"data-ascii.txt\"\n",
        "# HINT: ref [2]\n",
        "with open(\"data.exercise.M.1.ascii.txt\", 'w') as outfile:\n",
        "    for name in translit_names:\n",
        "        outfile.write(name + \"\\n\")\n",
        "#3.9\n",
        "# Verify that the names were converted and written out correctly\n",
        "with open(\"data.exercise.M.1.ascii.txt\", 'r') as infile:\n",
        "    for line in infile:\n",
        "        print(line.rstrip())"
      ],
      "id": "17tX_qpT4l4F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your reflection here\n",
        "#3.1\n",
        "#3.2\n",
        "#3.3\n",
        "#3.4\n",
        "#3.5\n",
        "#3.6\n",
        "#3.7\n",
        "#3.8\n",
        "#3.9\n"
      ],
      "metadata": {
        "id": "X6tBzbFR2f_v"
      },
      "id": "X6tBzbFR2f_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmOzzRTl4l4P"
      },
      "source": [
        "**`Expected Output`**\n",
        "```\n",
        "Richard Phillips Feynman\n",
        "Shin'ichiro Tomonaga\n",
        "Julian Schwinger\n",
        "Rudolf Ludwig Moessbauer\n",
        "Erwin Schroedinger\n",
        "Paul Dirac\n",
        "Maria Sklodowska-Curie\n",
        "Pierre Curie\n",
        "```\n",
        "\n"
      ],
      "id": "zmOzzRTl4l4P"
    },
    {
      "cell_type": "markdown",
      "id": "5f09fcd7",
      "metadata": {
        "id": "5f09fcd7"
      },
      "source": [
        "## **`assign.M.1.assignment.1`** - Data Mining with Covid Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`Overview and Directions`**"
      ],
      "metadata": {
        "id": "ujMShg_DwKdx"
      },
      "id": "ujMShg_DwKdx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import and manipulate a .csv file  \n",
        "2. Assess your Python Programming Skills  \n",
        "3. Other assignments are more challenging; use this to assess your skills.\n",
        "4. Prepare questions for a class discussion to help source additional tools. \n",
        "5. Perform tasks without assistance from clever sources.    \n",
        "\n",
        "#### **`Desired outcomes`**  \n",
        "- Experience Pandas dataframes to group, aggregate, find, sort, and calculate.  \n",
        "- Perform calculations to find best country, rank, and total items processed. \n",
        "- Note: Pandas is reviewed in Module 2 and quality resources provided below.  \n",
        "\n",
        "#### **`Additional resources`**  \n",
        "- [Daniel Chen](https://github.com/chendaniely/) is a **generous** Pandas master.  \n",
        "=> Purchase of his books is recommended; not a solicitation!    \n",
        "- [Chen,D.,(2022). Pandas for everyone, 2nd.Ed.](https://www.amazon.com/Pandas-Everyone-Analysis-Addison-Wesley-Analytics/dp/0137891156/ref=sr_1_1?crid=T9BF3HU24YFL&keywords=pandas+for+everyone&qid=1685205022&sprefix=pandas+for+everyone%2Caps%2C203&sr=8-1)  \n",
        "=> [groupby](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/07-groupby.ipynb) => [missing values](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/03-missing.ipynb) => [many more!](https://github.com/chendaniely/2017-10-26-python_crash_course/tree/gh-pages/notebooks)    \n",
        "\n"
      ],
      "metadata": {
        "id": "orcAiqUZAQ6Y"
      },
      "id": "orcAiqUZAQ6Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`Task.0`**  \n"
      ],
      "metadata": {
        "id": "RNiLo5Rm9SM2"
      },
      "id": "RNiLo5Rm9SM2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Dataset`**\n",
        "- COVID-19 variant [sequencing](https://www.cdc.gov/coronavirus/2019-ncov/variants/genomic-surveillance.html#:~:text=Scientists%20use%20a%20process%20called%20genomic%20sequencing%20to%20identify%20SARS,test%20positive%20for%20COVID%2D19) by countries.   \n",
        "Data fields    \n",
        "1. `location`: the country providing information.    \n",
        "2. `date`: data entry date.  \n",
        "3. `variant`: the COVID-19 variant for the entered record.  \n",
        "4. `num_sequences`: the number of sequences **processed** by country, variant, and date.   \n",
        "5. `num_sequences_total`: the number of sequences **available** by country, variant, and date.  \n",
        "6. `perc_sequences`: the percentage of the available sequences processed (*out of 100*)  \n",
        "`note:` each dataset row represents *one* variant by *one* country on *one* day.  \n",
        "\n",
        "**Tasks**  \n",
        "1. Locate and read dataset into a pandas.DataFrame called 'df' via  \n",
        "a. A Kaggle API; use existing or acquire; [Kaggle.covid.dataset](https://www.kaggle.com/yamqwe/omicron-covid19-variant-daily-cases?select=covid-variants.csv)  \n",
        "or  \n",
        "b. Class github URL or another .csv method like [Matthes, Ch.16](https://github.com/cosc-526/cosc.526.home.page/blob/main/textbook.Python.crash.course.matthes.pdf)    \n",
        "=> filename: **data.assignment.M.1.covid.data.csv**  \n",
        "=>**consider** reading a Github data URL requires a path to **raw data**    \n",
        "2. Display the DataFrame's first 5 rows.  \n",
        "3. Display descriptive stats confirming: 100,416 data records.  \n",
        "4. Round DataFrame to 1 decimal place!   \n",
        "\n",
        "**Useful links**  \n",
        "[Built-in Functions](https://docs.python.org/3/library/functions.html#built-in-functions)  \n",
        "[pandas.DataFrame documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) "
      ],
      "metadata": {
        "id": "wcrKuRsWXUW1"
      },
      "id": "wcrKuRsWXUW1"
    },
    {
      "cell_type": "code",
      "source": [
        "## Enter solution here\n"
      ],
      "metadata": {
        "id": "J6JeWBO2Cw78"
      },
      "id": "J6JeWBO2Cw78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Task.0 - Expected Outcome`**  \n",
        "```\n",
        "DataFrame header\n",
        "  location  date        variant  num_sequences  perc_sequences  num_sequences_total\n",
        "0   Angola  2020-07-06      Alpha              0             0.0   3\n",
        "1   Angola  2020-07-06  B.1.1.277              0             0.0   3\n",
        "2   Angola  2020-07-06  B.1.1.302              0             0.0   3\n",
        "3   Angola  2020-07-06  B.1.1.519              0             0.0   3\n",
        "4   Angola  2020-07-06    B.1.160              0             0.0   3\n",
        "\n",
        "Dataframe descriptive statistics, rounded to tenths\n",
        "       num_sequences  perc_sequences  num_sequences_total\n",
        "count       100416.0        100416.0             100416.0\n",
        "mean            72.0             6.0               1510.0\n",
        "std           1669.0            22.0               8445.0\n",
        "min              0.0            -0.0                  1.0\n",
        "25%              0.0             0.0                 12.0\n",
        "50%              0.0             0.0                 59.0\n",
        "75%              0.0             0.0                394.0\n",
        "max         142280.0           100.0             146170.0 \n",
        "```\n"
      ],
      "metadata": {
        "id": "4z6COkzkC2KU"
      },
      "id": "4z6COkzkC2KU"
    },
    {
      "cell_type": "markdown",
      "id": "760c5a8f",
      "metadata": {
        "id": "760c5a8f"
      },
      "source": [
        "### **`Task.1`** - Find uncommon variants  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The U.S. experienced the COVID-19 `Alpha`, `Delta`, `Omicron`  \n",
        "\n",
        "**Tasks**  \n",
        "0. In whatever object you like, e.g. list, dataframe, etc  \n",
        "1. Get unique variant items for category: **`US_and_other`**  \n",
        "=> where variants == [US, `non_who`, `others`]  \n",
        "2. Get unique variant items for category: **`nonUS_and_other`**  \n",
        "=> where variants != [US, `non_who`, `others`]  \n",
        "3. Print your chosen objects to display unique variant categories.  \n",
        "4. Show a total unique count for each, and total for dataset,\n",
        "\n",
        "**Useful links**   \n",
        "- [len()](https://docs.python.org/3/library/functions.html#len)\n",
        "- [list comprehension w Bro Code](https://www.youtube.com/watch?v=fcLDzKH_5XM)  "
      ],
      "metadata": {
        "id": "ZGy_VBO4DDyd"
      },
      "id": "ZGy_VBO4DDyd"
    },
    {
      "cell_type": "code",
      "source": [
        "#=>Enter Your Solution\n"
      ],
      "metadata": {
        "id": "vxMduRg5Cj5u"
      },
      "id": "vxMduRg5Cj5u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Task.1 - Expected Outcome`**  \n",
        "```\n",
        "note: organization of output can vary widely!  \n",
        "\n",
        "['Alpha', 'Delta', 'Omicron', 'others', 'non_who']  \n",
        "\n",
        "total US + other =  5\n",
        "\n",
        "['B.1.1.277', 'B.1.1.302', 'B.1.1.519', 'B.1.160', 'B.1.177', 'B.1.221',  \n",
        " 'B.1.258', 'B.1.367', 'B.1.620', 'Beta', 'Epsilon', 'Eta', 'Gamma', 'Iota',  \n",
        "  'Kappa', 'Lambda', 'Mu', 'S:677H.Robin1', 'S:677P.Pelican']   \n",
        "  \n",
        "total nonUS+other =  19   \n",
        "\n",
        "total unique variants =  24 \n",
        "```"
      ],
      "metadata": {
        "id": "jBOVMJ0zck9a"
      },
      "id": "jBOVMJ0zck9a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`Task.2`** - Find the most processed variant  "
      ],
      "metadata": {
        "id": "aB6-dtiH5OSq"
      },
      "id": "aB6-dtiH5OSq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks**  \n",
        "1. Which variant of COVID-19 has the most sequences processed?  \n",
        "2. Store and print the result in a string called **`variant_most_proc`**  \n",
        "\n",
        "**Useful links**  \n",
        "[pd.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas-dataframe-groupby), [pd.DataFrame.aggregate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html#pandas-dataframe-aggregate)"
      ],
      "metadata": {
        "id": "bRbxoQb9rbN9"
      },
      "id": "bRbxoQb9rbN9"
    },
    {
      "cell_type": "code",
      "source": [
        "#=>Enter Your Solution\n"
      ],
      "metadata": {
        "id": "JoFZ2N-WFDH1"
      },
      "id": "JoFZ2N-WFDH1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Task.2 - Expected Outcome`**  \n",
        "```\n",
        "Delta  \n",
        "```"
      ],
      "metadata": {
        "id": "5uAWiinsdBi2"
      },
      "id": "5uAWiinsdBi2"
    },
    {
      "cell_type": "markdown",
      "id": "663db8ac",
      "metadata": {
        "id": "663db8ac"
      },
      "source": [
        "### **`Task.3`** - Find the best country at processing ALL variant sequences  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks**  \n",
        "1. Which country did the best processing **all** categories.    \n",
        "2. Store the result in a string called **`best_proc_country`**  \n",
        "3. The outcome is a single country.  \n",
        "4. **consider** df.groupby(\"location\").aggregate({\"num_sequences\": \"sum\", \"num_sequences_total\": \"sum\"})\n",
        "\n",
        "**Useful links**  \n",
        "[youtube: aggregate with groupby and .agg or .aggregate](https://www.youtube.com/watch?v=PNzlx3CjqAE)\n"
      ],
      "metadata": {
        "id": "ny95wr5_sQhK"
      },
      "id": "ny95wr5_sQhK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c31b41",
      "metadata": {
        "id": "75c31b41"
      },
      "outputs": [],
      "source": [
        "#=>Enter Your Solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Task.3 - Expected Outcome`**\n",
        "```\n",
        "Total percent performed by country \n",
        "\n",
        "location                percent\n",
        "Cyprus                  8.19\n",
        "Hungary                 7.96\n",
        "Egypt                   7.77\n",
        "United Arab Emirates    7.58\n",
        "Uruguay                 7.26\n",
        "                        ... \n",
        "Seychelles              4.25\n",
        "Fiji                    4.23\n",
        "Slovakia                4.23\n",
        "Brunei                  4.22\n",
        "Vietnam                 4.18\n",
        "Name: perc_sequences, Length: 121, dtype: float64 \n",
        "\n",
        "the best country is =>  Cyprus\n",
        "```"
      ],
      "metadata": {
        "id": "y0bRJhZLdKBu"
      },
      "id": "y0bRJhZLdKBu"
    },
    {
      "cell_type": "markdown",
      "id": "11d1d3a0",
      "metadata": {
        "id": "11d1d3a0"
      },
      "source": [
        "### **`Task.4a`** - Find the best country at processing specific variant sequences  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks**  \n",
        "1. Which country is best at processing sequences for Alpha, Delta, and Omicron variants?    \n",
        "2. Store and print the result in a string called **`best_proc_country_ADO`** \n",
        "3. The final output is a single country.  \n",
        "\n",
        "**Useful links** \n",
        "- ibid  \n",
        "\n"
      ],
      "metadata": {
        "id": "Jxd-0IN00go_"
      },
      "id": "Jxd-0IN00go_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcafdd2e",
      "metadata": {
        "id": "fcafdd2e"
      },
      "outputs": [],
      "source": [
        "#=>Enter Your Solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Task.4a - Expected Outcome`**  \n",
        "```\n",
        "best_proc_country_ADO = Vietnam  \n",
        "```"
      ],
      "metadata": {
        "id": "fvANdWH2kOqc"
      },
      "id": "fvANdWH2kOqc"
    },
    {
      "cell_type": "markdown",
      "id": "9ccb8782",
      "metadata": {
        "id": "9ccb8782"
      },
      "source": [
        "### **`Task.4b`** - Find the United States ranking for processing Alpha, Delta, and Omicron  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks**  \n",
        "Given the outcome in 4a\n",
        "1. Find the positional index value for the US ranking for processing sequences for Alpha, Delta, an Omicron variants.   \n",
        "2. Store and print the ranking as an integer in a **us_ranking** variable.  \n",
        "3. Ensure your ranking scale reflects a scale starting at 1.  \n",
        "4. As a refresher, Python indexing starts at 0.  \n",
        "\n",
        "**Useful links** \n",
        "- [enumerate](https://docs.python.org/3/library/functions.html#enumerate)  "
      ],
      "metadata": {
        "id": "uS-CWHXDFnlB"
      },
      "id": "uS-CWHXDFnlB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038e81b5",
      "metadata": {
        "id": "038e81b5"
      },
      "outputs": [],
      "source": [
        "#=>Enter Your Solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Task.4b - Expected Outcome`**  \n",
        "```\n",
        "United States ranking = 57  \n",
        "```"
      ],
      "metadata": {
        "id": "G5SUmnTckQBM"
      },
      "id": "G5SUmnTckQBM"
    },
    {
      "cell_type": "markdown",
      "id": "283e46d3",
      "metadata": {
        "id": "283e46d3"
      },
      "source": [
        "### **`Task.5.<final.task> - Write instructions for a jr. data scientist assignment`**\n",
        "**`Task =>`**  \n",
        "- Write clear and precise directions that enable your  new junior  \n",
        "- data analyst, aka \"Jr,\" to modify and fix code that you provide.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **`Grading requirements=>`**  \n",
        "A clear and precise explanation of specific activities for production code your boss needs but you dont have time to fix.  "
      ],
      "metadata": {
        "id": "913dpFR0Fp1E"
      },
      "id": "913dpFR0Fp1E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data science requires clear explanations of tasks, methodology, and effective communication with peers. To help the new junior analyst complete their first assignment, provide a concise and precise description including  \n",
        "\n",
        "1. `Sample Outcome`\n",
        "Deliver a comprehensive report summarizing findings and insights from data analysis. Include, as needed, desired outcome format, data objects, and visualizations.  \n",
        "\n",
        "2. `Python Code Explanation`\n",
        "Use plain language to describe specific Python code to achieve the desired outcome. Refer to pandas, Python, and other library documentation to incorporate particular language.  \n",
        "\n",
        "3. `Consider Deprecated Functions`\n",
        "The provided code is outdated and broken. Encourage problem-solving skills and leverage previous experience with similar tasks. Provide relevant links for reverse engineering.  \n",
        "\n",
        "**`Additional personnel considerations`**\n",
        "4. `Plain Language Explanation`\n",
        "Consider the junior analyst's background in C and provide clear and unambiguous instructions.  \n",
        "\n",
        "5. `Documentation Reference`\n",
        "Emphasize where to consult pandas, Python, and other library documentation to discern code mechanics and clarify concepts.  \n",
        "\n",
        "---------------"
      ],
      "metadata": {
        "id": "2mN_TFHdT1Vg"
      },
      "id": "2mN_TFHdT1Vg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Your manager's original request => [memo substrate]` \n",
        "\n",
        "`\"hey! i need by lunch` the processed sequences per country on any date`  \n",
        "because as CFO wants to crunch numbers this afternoon - thx Lambda\"\n",
        "\n",
        "1. Determine the percentage of processed sequences for the Alpha, Delta, and Omicron variants in the US.  \n",
        "2. Store the result as a dictionary where keys are variant names and values are percentages.  \n",
        "3. Save in variable = proc_seq_us\n",
        "\n",
        "`=> Other implied items based on same exercise for manager last year`\n",
        "- Determine each country's total processed sequences for Omicron on December 27, 2021 or any other date entered (date updated from 2020).\n",
        "- provide country name and # processed sequences\n",
        "- bidirectional sorting\n",
        "- store outcomes in tuple like mytuple(country_name, processed_sequen, ) \n",
        "- variables totals like `total_omicron_2021`  "
      ],
      "metadata": {
        "id": "TOdZMTe-Rcl_"
      },
      "id": "TOdZMTe-Rcl_"
    },
    {
      "cell_type": "code",
      "source": [
        "#=> Enter Your Solution; i.e. write memo here\n"
      ],
      "metadata": {
        "id": "sMFoLYaXlRGZ"
      },
      "id": "sMFoLYaXlRGZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `5a - code you found from last year's excercise `"
      ],
      "metadata": {
        "id": "uV6K5AkAfDET"
      },
      "id": "uV6K5AkAfDET"
    },
    {
      "cell_type": "code",
      "source": [
        "#=> I of III - broken code last year\n",
        "\n",
        "total_omicron_2021 = []\n",
        "#5.1\n",
        "df = df.set_index(\"location\")\n",
        "#5.2\n",
        "df = df.loc[df6[\"date\"] == \"2021-12-27\"]\n",
        "#5.3\n",
        "df = df6.loc[df6[\"variant\"] == \"Omicron\"]\n",
        "#5.3\n",
        "df = df6[\"num_sequences\"]\n",
        "#5.4\n",
        " = list(zip(df.index, df))\n",
        "#5.5\n",
        "df7 = pd.DataFrame(sorted(missing, key=lambda x: x[1], reverse=True))\n",
        "print(df7)"
      ],
      "metadata": {
        "id": "qUkhMZD8T40i"
      },
      "id": "qUkhMZD8T40i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **`5a - Expected Outcome`**  \n"
      ],
      "metadata": {
        "id": "dzXyv7kGZlBP"
      },
      "id": "dzXyv7kGZlBP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "0\t                1\n",
        "0\tUnited Kingdom\t52456\n",
        "1\tUnited States\t  24681\n",
        "2\tDenmark\t        3331\n",
        "3\tGermany\t        1701\n",
        "4\tIsrael\t        1578\n",
        "...\t...\t...\n",
        "59\tVietnam\t      1\n",
        "60\tMoldova\t      0\n",
        "61\tMonaco\t      0\n",
        "62\tNepal\t        0\n",
        "63\tSouth Korea \t0\n",
        "64 rows × 2 columns\n",
        "```"
      ],
      "metadata": {
        "id": "g1qcVGxqavtv"
      },
      "id": "g1qcVGxqavtv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `5b - code you found from last year's excercise `"
      ],
      "metadata": {
        "id": "5G3szw10bNTr"
      },
      "id": "5G3szw10bNTr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cf6efa9",
      "metadata": {
        "id": "6cf6efa9"
      },
      "outputs": [],
      "source": [
        "#=> II of III - broken code last year\n",
        "\n",
        "proc_seq_us = {}\n",
        "df2 = df.groupby([\"location\", \"variant\"]).aggregate({\n",
        "    \"num_sequences\": \"sum\",\n",
        "    \"num_sequences_total\": \"sum\",\n",
        "})\n",
        "df2[\"perc_sequences\"] = (df2[\"num_sequences\"] / df2[\"num_sequences_total\"]) * 100\n",
        "df2 = df2.loc[(\"United States\", [\"Alpha\", \"Delta\", \"Omicron\"]), :].loc[\"United States\"]\n",
        "df2 = df2[\"perc_sequences\"]\n",
        "proc_seq_us = df2.to_dict()\n",
        "print(proc_seq_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **`5b - Expected Outcome`**  \n"
      ],
      "metadata": {
        "id": "NfYB_LjbZ_zq"
      },
      "id": "NfYB_LjbZ_zq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{'Alpha': 11.520951617373877, 'Delta': 63.76796208057254, \n",
        "                                                'Omicron': 1.370817855027461}\n",
        "```"
      ],
      "metadata": {
        "id": "_TI3iusnbUbO"
      },
      "id": "_TI3iusnbUbO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `5c - code you found from last year's excercise `"
      ],
      "metadata": {
        "id": "ySKYi-77bbXc"
      },
      "id": "ySKYi-77bbXc"
    },
    {
      "cell_type": "code",
      "source": [
        "#=> III of III - broken code last year\n",
        "\n",
        "total_omicron_2021 = []\n",
        "#5.1\n",
        "df6 = df.set_index(\"location\")\n",
        "#5.2\n",
        "df6 = df6.loc[df6[\"date\"] == \"2021-12-27\"]\n",
        "#5.3#\n",
        "df6 = df6.loc[df6[\"variant\"] == \"Omicron\"]\n",
        "#5.3\n",
        "df6 = df6[\"num_sequences\"]\n",
        "#5.4\n",
        "total_omicron_2021 = list(zip(df6.index, df6))\n",
        "#5.5\n",
        "df7 = pd.DataFrame(sorted(total_omicron_2021, key=lambda x: x[1], reverse=True))\n",
        "print(df7)\n",
        "total_omicron_2021 = sorted(total_omicron_2021, key=lambda x: x[1], reverse=True)\n",
        "print(total_omicron_2021)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilcLFMXTW9jG",
        "outputId": "2b827e05-e959-4580-861d-518facb7c1c3"
      },
      "id": "ilcLFMXTW9jG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 0      1\n",
            "0   United Kingdom  52456\n",
            "1    United States  24681\n",
            "2          Denmark   3331\n",
            "3          Germany   1701\n",
            "4           Israel   1578\n",
            "..             ...    ...\n",
            "59         Vietnam      1\n",
            "60         Moldova      0\n",
            "61          Monaco      0\n",
            "62           Nepal      0\n",
            "63     South Korea      0\n",
            "\n",
            "[64 rows x 2 columns]\n",
            "[('United Kingdom', 52456), ('United States', 24681), ('Denmark', 3331), ('Germany', 1701), ('Israel', 1578), ('Australia', 1319), ('Switzerland', 514), ('France', 509), ('Italy', 486), ('Belgium', 464), ('Spain', 461), ('Sweden', 434), ('Chile', 260), ('Netherlands', 254), ('Singapore', 249), ('Mexico', 240), ('Turkey', 202), ('India', 174), ('Brazil', 147), ('Botswana', 142), ('Indonesia', 128), ('Japan', 118), ('Portugal', 118), ('Argentina', 80), ('New Zealand', 63), ('South Africa', 61), ('Lithuania', 50), ('Czechia', 49), ('Georgia', 46), ('Russia', 45), ('Colombia', 37), ('Sri Lanka', 37), ('Hong Kong', 35), ('Malta', 34), ('Poland', 28), ('Ecuador', 26), ('Canada', 25), ('Jordan', 22), ('Malawi', 21), ('Cambodia', 18), ('Norway', 17), ('Morocco', 15), ('Senegal', 15), ('Costa Rica', 14), ('Pakistan', 11), ('Nigeria', 10), ('Peru', 10), ('Brunei', 8), ('Slovakia', 8), ('Trinidad and Tobago', 8), ('Maldives', 7), ('Zambia', 7), ('Thailand', 6), ('Malaysia', 5), ('Bangladesh', 4), ('Romania', 3), ('Iran', 1), ('Oman', 1), ('Ukraine', 1), ('Vietnam', 1), ('Moldova', 0), ('Monaco', 0), ('Nepal', 0), ('South Korea', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **`5c - Expected Outcome`**  \n"
      ],
      "metadata": {
        "id": "x3c3REubaSEx"
      },
      "id": "x3c3REubaSEx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "                 0      1\n",
        "0   United Kingdom  52456\n",
        "1    United States  24681\n",
        "2          Denmark   3331\n",
        "3          Germany   1701\n",
        "4           Israel   1578\n",
        "..             ...    ...\n",
        "59         Vietnam      1\n",
        "60         Moldova      0\n",
        "61          Monaco      0\n",
        "62           Nepal      0\n",
        "63     South Korea      0\n",
        "[64 rows x 2 columns]\n",
        "\n",
        "#[('United Kingdom', 52456), ('United States', 24681), ('Denmark', 3331),\n",
        " ('Germany', 1701), ('Israel', 1578), ('Australia', 1319), ('Switzerland', 514),\n",
        "  ('France', 509), ('Italy', 486), ('Belgium', 464), ('Spain', 461), \n",
        "  ('Sweden', 434), ('Chile', 260), ('Netherlands', 254), ('Singapore', 249),\n",
        "  ('Mexico', 240), ('Turkey', 202), ('India', 174), ('Brazil', 147),\n",
        "   ('Botswana', 142), ('Indonesia', 128), ('Japan', 118), ('Portugal', 118),\n",
        "    ('Argentina', 80), ('New Zealand', 63), ('South Africa', 61), \n",
        "    ('Lithuania', 50), ('Czechia', 49), ('Georgia', 46), ('Russia', 45), \n",
        "    ('Colombia', 37), ('Sri Lanka', 37), ('Hong Kong', 35), ('Malta', 34),\n",
        "     ('Poland', 28), ('Ecuador', 26), ('Canada', 25), ('Jordan', 22), \n",
        "     ('Malawi', 21), ('Cambodia', 18), ('Norway', 17), ('Morocco', 15), \n",
        "     ('Senegal', 15), ('Costa Rica', 14), ('Pakistan', 11), ('Nigeria', 10),\n",
        "      ('Peru', 10), ('Brunei', 8), ('Slovakia', 8), ('Trinidad and Tobago', 8),\n",
        "       ('Maldives', 7), ('Zambia', 7), ('Thailand', 6), ('Malaysia', 5), \n",
        "       ('Bangladesh', 4), ('Romania', 3), ('Iran', 1), ('Oman', 1),\n",
        "        ('Ukraine', 1), ('Vietnam', 1), ('Moldova', 0), ('Monaco', 0), \n",
        "        ('Nepal', 0), ('South Korea', 0)]\n",
        "```"
      ],
      "metadata": {
        "id": "2JNdnFwTap_f"
      },
      "id": "2JNdnFwTap_f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `assignment.M1.Housekeeping`"
      ],
      "metadata": {
        "id": "kjA00fVqPjEm"
      },
      "id": "kjA00fVqPjEm"
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "\n",
        "##=>Perform A,B,C when done with spark\n",
        "#A: \n",
        "#=> Close the SparkSession\n",
        "spark.stop()\n",
        "\n",
        "#B:\n",
        "#=> Disconnect and stop Spark in a Jupyter Notebook,\n",
        "#=> stops SparkContext and releases its resourcs\n",
        "sc.stop()\n",
        "\n",
        "#C: \n",
        "#=> Confirm Spark termination by checking the Spark UI\n",
        "#=> Access UI by visiting URL provided in Notebook output where Spark fireup"
      ],
      "metadata": {
        "id": "2DzgneXkPhuC"
      },
      "id": "2DzgneXkPhuC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `<end.M1` ~ `M1.end>`\n",
        "-------------"
      ],
      "metadata": {
        "id": "QuEPQ9tfwvRH"
      },
      "id": "QuEPQ9tfwvRH"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jBOVMJ0zck9a",
        "283e46d3",
        "913dpFR0Fp1E",
        "uV6K5AkAfDET",
        "5G3szw10bNTr",
        "NfYB_LjbZ_zq",
        "ySKYi-77bbXc",
        "QB_KhbotUukr",
        "CU3PQXM0GCkO",
        "ea7scL-NEmCR",
        "h6DHf8eXEmCu",
        "zl_trypLEmCv",
        "TRzsydJjKB7b",
        "Hn4Iuwq2L36e",
        "XUoNyI0_gfe2",
        "a4PeBkYJ6L95",
        "ogwwvpsJ6MGI",
        "zuvgtcMS6MSB",
        "m_o-TAG56MWo",
        "rG70DXQn6MZ8",
        "iBNcMUha6MdA",
        "5_2pMZkZLGRk",
        "lMnLmuCsLGSH",
        "OLCAqU4dLGSL",
        "IkUXuPexCope",
        "YsERDkCmn5i3",
        "s2SPQ2NZkpah",
        "ds7q2xQmnP5t",
        "4ibLZdl_qfm9",
        "kFPFbUc0qaZ7",
        "5OQl4IjEnZLz",
        "UtEKPHyQt_9R",
        "ZIrZKxkck5P_",
        "r2DcADR4kfFo"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}