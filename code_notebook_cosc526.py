# -*- coding: utf-8 -*-
"""code_notebook_cosc526.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nGU7tGnyfBUtpZ_tUVDsw0DkD9QbPwqP

# **`M.0.Overview of codebook, answers, instructions, and libraries`**

## **`I. how.To use course codebook`**  
1. Notebook works in both Jupyter Notebooks & Colab.  
2. Google built [colab](https://colab.research.google.com/) on top of Notebooks to add more features.  
3. Cosc.526 was configured in Colab with expand/collapse sectioning.  
4. There are 9 parent sections for each course module.  
5. Modules contain either exercise and assignment or both.  
6. **ALL** additional links and resources are to quality resources and tools.  
=> Consider bookmarking them for future analysis activities!  
7. Your going to be "coding." Consider flipping a monitor vertically to help code.    
8. A minimum of two monitors is recommended.   
9. Try open 2 workbooks at same time on different monitors!
10. **p.s.** colab offers screen companions like sparks, corgis, and crabs

## **`II. What qualifies as a task answer?`**  
- `Answer:` answers are outcomes generated from code that executes.
- when you generate a .pdf, simply dont delete answer beneath ur code run.
- ***What if your task's code isn't running?***  
- Add one textbox beneath block titled ***Enter solution here***  
- Add title: "Professor: explanation of current state"
- Succinctly explain work done (for grading points).  

## **`III. Umbrella instructions`**  
1. ### **`How do I solve exercise and assignment tasks?`**  
a. by writing code.  
b. by modifying code provided.  
c. by answering specified questions concisely.  
d. as necessary, reference any resource that helped accomplish work.

2. ### **`What are the submission requirements?`**  
d. `Answer:` upload a **.pdf** copy after renaming it to...    
e. => weeks 1,2: last.first.exercise.and.assignment.M.#.pdf  
f. => weeks 3-9: last.first.assignment.M.#.pdf  

3. ### **`How do I generate a .pdf to submit work?`**
g. `Answer:` Jupyter Notebook has File\Save As\ adobe.pdf  
h. **What do you do if .pdf generation fails?**  
i. task.1) when in doubt, always submit a **.ipynb** file!  
j. task.2) inform professor requesting permission to submit **.ipynb**   
k. task.3) research and engineer the fix; Youtube and [Jupyter.forum](https://discourse.jupyter.org/).

4. ### **`What is an Expected Outcome section in tasks below?`**  
l. `Answer:` Its a solution to a provided task to help guide your efforts.   
m. They're regularly provided, but only sometimes.   
n. They're located directly beneath codeblock=> **Enter solution here**  

5. ### **`How many code blocks should my output answer be?`**    
o. `Answer` as few as possible which means ideally no more than "1"  
=> Plan and organize final outcomes. Brevity is key!  
=> points will be "detracted" for unorganized and unprofessional codeblocks  

## **`IV. cosc.526 Python required libraries`**  
1. Library engineering assistance is next to help get you running now.  
2. Confirm library code executes completly; `V. Library Resources`  
3. If not, research and solve right away! [Jupyter.forum](https://discourse.jupyter.org/)  
4. **`Why?`** Errors occur for many reasons and can be challenging to fix.    
3. Use source library document pages for additional assistance and [pypi.org](https://pypi.org/)

# **`V. Library Resources`**     
1. Python coding companion => [Python reference.library.Cosc.526.pdf](https://github.com/cosc-526/cosc.526.home.page/blob/main/reference.library.COSC.526.pdf) <=`print.ME`
3. Use [Python Docs;](https://docs.python.org/3/) **`they're simply the best.`** [TinaTurner](https://www.youtube.com/watch?v=GC5E8ie2pdM) we love you.

## `They're many reasons for library initialization errors`
- #### research and fix using quality key word searchers like
- #### `git`, `docs`, `colab`, `error`, paste error into search  
- #### recommendation: avoid "news" type websites
- #### recommendation: google provides optimized data science outcomes

- **`numpy as np`**  badass scientific for multi-dimensional arrays and math functions.
- **`pandas as pd`**  spreadsheet like data manipulation and analysis.
- **`os`** talk to your op system and nav directories, files, and sys commands.
- **`matplotlib.pyplot as plt`** for plotting and visualization.
- **`pyspark`** API for Apache Spark for processing large-scale datasets in parallel.
- **`keras`** neural networks API on top TensorFlow for deep learning w images and NLP.

## `Engineered install convience window :>)`
"""

#=> why provided? 
#=> you so you can focus on reading all LMS links and resources

# Are needed libraries installed?
# => on.jupyter:  !pip show  <library.name> 
# => on.terminal:  pip show scikit-learn

# how.To Install library
# => on.jupyter:  !pip install  <library.name> 
# => on.terminal:  pip install scikit-learn
# => package manager => https://pypi.org

#=> New to data mining? install all!
#!pip install numpy
#!pip install pandas
#!pip install pyspark
#!pip isntall tensorflow
#!pip install keras
#!pip install matplotlib
#!pip install scikit-learn

#=> Example of quality sources
#https://scikit-learn.org/stable/install.html
#https://spark.apache.org/docs/latest/api/python/getting_started/install.html
#https://dvgodoy.github.io/handyspark/includeme.html#:~:text=HandySpark%20is%20a%20package%20designed,returning%20pandas%20objects%20straight%20away.

#=> how.To update all installed libraries ?
#thanks! rbergeron@snhu.edu
#python3 -m pip list --outdated --format=json | jq -r '.[] | "\(.name)==\(.latest_version)"' | xargs -n1 pip3 install -U

#misc
# python -m pip freeze          # what are all installed packages in virtualenv

import pandas as pd
import numpy as np
import os

import matplotlib.pyplot as plt
import plotly                #SVM hyperplane graphing
import plotly.express as px  #SVM hyperplane graphing

#unsupervised
import sklearn
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler #kind of important
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import TruncatedSVD
from sklearn.manifold import TSNE

#supervised
from sklearn.linear_model import Perceptron
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
print("library.wheels.up")

"""### PySpark: [Interstellar: we're going to spark it](https://www.google.com/search?sxsrf=APwXEdfIje02_Nln52w2MKqzRUz6KA3sBA:1685806695619&q=interstellar+were+going+to+spark+it&tbm=isch&sa=X&ved=2ahUKEwjG_4Gqt6f_AhUMEFkFHaG8BfcQ0pQJegQINxAB&biw=1200&bih=2010&dpr=0.9#imgrc=M4cdnQpK79fKhM)"""

!pip install pyspark
#!pip show pyspark

#=> Option A
import tensorflow as tf
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local").getOrCreate() #create spark session
#"local" argument passed to master; run spark in local mode on Colab runtime.

from pyspark.sql import SparkSession #note: if run notebooks, need Java
import pyspark.sql.functions as F
import matplotlib.pyplot as plt
from pyspark.sql.types import *
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import LogisticRegressionTrainingSummary
from pyspark.ml.classification import LogisticRegressionModel, LogisticRegressionSummary
from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer
from pyspark.ml.feature import PCA
from pyspark.ml.linalg import Vectors
from pyspark.ml.classification import NaiveBayes  #Perceptron, 
from pyspark.ml.classification import DecisionTreeClassifier, LinearSVC
from pyspark.ml.classification import RandomForestClassifier, GBTClassifier
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.clustering import KMeans
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
print("spark.library.wheels.up")

#=> when done with spark goto end of assignment & close spark; run 
#spark.stop()  <optional:  run sc.stop()>

"""#### `=> Pyspark discrete setting option for Jupyter Notebook, not Colab`

- import handyspark: additional functionalities on top of PySpark like (?)  
- import findspark: locates the Spark install sets environment variables.  
- findspark.init(): ibid  
- from pyspark import SparkContext, SparkConf  
=> pyspark classes create a SparkContext object; Colab does automatically  
`note:` Jupyter Notebook requires `Java;` Colab prebuilt with
```
#=> Option B: Notebook discrete performance settings
import handyspark, findspark
findspark.init()
from pyspark import SparkContext, SparkConf
sc = SparkContext(master="local[4]")        # .o.n.l.y. run ONCE
>>> sc => example how to create a SparkContext with a local master and 4 cores. 
```

# **`M.1.Introduction to data mining`**

## **`exercise.M.1`** - Python Warmup

### **`Overview and Directions`**

* Practice importing and parsing information. 
* Focus on learning and solving versus coding perfectly.  
* Perform tasks without assistance from clever sources.  

#### **`Desired outcomes`**  
- Experience open, read, and writing of external delimited files.  
- Navigate basic text mining preprocessing like white space stripping.  
- Refresh expereience with iterator, conditionals, and functions.  

####**`Additional Resources`**  
- course [Python reference.library.Cosc.526.pdf](https://github.com/cosc-526/cosc.526.home.page/blob/main/reference.library.COSC.526.pdf)  
=> contains a deep and wide index of essential Python coding.  
- Bookmark the masters of text preprocessing  
=> [Jurafsky and Martin, Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)  
=> additional quality textbook on classical text mining  
=> [Wiess,S.,Indurkhya,N.,Zhang,T.,(2015), Fundamentals of predictive text mining, 2nd, Springer.](https://www.amazon.com/Fundamentals-Predictive-Mining-Computer-Science/dp/144716749X/ref=sr_1_1?crid=MUA7UG21IFPD&keywords=Wiess%2CS.%2CIndurkhya%2CN.%2CZhang%2CT.%2C%282015%29%2C+Fundamentals+of+predictive+text+mining%2C+2nd%2C+Springer&qid=1685151591&sprefix=wiess%2Cs.%2Cindurkhya%2Cn.%2Czhang%2Ct.%2C+2015+%2C+fundamentals+of+predictive+text+mining%2C+2nd%2C+springer%2Caps%2C78&sr=8-1)

### **`Task.1`**  - comma-separated values (.csv)

Reading and parsing [delimiter-separated values](https://en.wikipedia.org/wiki/Delimiter-separated_values) files like [comma-separated](https://en.wikipedia.org/wiki/Comma-separated_values) and [tab-separated values](https://en.wikipedia.org/wiki/Tab-separated_values) is a regular data science preprocessing activity. It is typically acceptable to request either file format for analysis activities.    
- *.csv* files store tabular data like numbers and text in a plain text format. 
- Plain text may include text, white spaces, carriage returns, transliterals, and other artifacts.    
- Each row, or data record, contains a value or nothing, and a comma separates each.    

**Tasks**  
0. [data.exercise.M.1.csv](https://github.com/cosc-526/cosc.526.home.page/blob/main/data.exercise.M.1.csv) => Nobel prize winners name and age     
1. Generate a single value for the total number of rows of data.
2. Generate a single value for the total number of columns of data.  
3. Calculate the laureates average age as a datatype float.  
4. **note:** the solution's answer is structured as a user-defined function (def).  
5. A def is not required, it contains extra code to use throughout the course.  
6. Recall, whenever feasible, combine answers into a single outcome code block.     

**Useful links**  
- [Ch.16, Importing Data, Python.Crash.Course, Matthes](https://github.com/cosc-526/cosc.526.home.page/blob/main/textbook.Python.crash.course.matthes.pdf)  
[open](https://docs.python.org/3.6/library/functions.html#open), 
[readlines](https://docs.python.org/3.6/library/codecs.html#codecs.StreamReader.readlines), [rstrip](https://docs.python.org/3.6/library/stdtypes.html#str.rstrip), [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions), [split](https://docs.python.org/3.6/library/stdtypes.html#str.split), [splice](https://docs.python.org/3.6/glossary.html#term-slice), ["list.love"](https://docs.python.org/3.6/tutorial/datastructures.html#more-on-lists), [len](https://docs.python.org/3.6/library/functions.html#len), [int](https://docs.python.org/3.6/library/functions.html#int), [format](https://docs.python.org/3.6/library/stdtypes.html#str.format)
"""

#=>Enter Your Solution

"""**Task.1 Expected Ouput**
```
Number of rows of data: 8
Number of cols: 3
Average Age: 70.875
```

### **`Task.2`** - tab-separated values (.tsv)

A [tab-separated value (.tsv)](https://en.wikipedia.org/wiki/Tab-separated_values) format is a delimiter-separated value for storing data in a tabular structure like a database table or spreadsheet. Other characteristics include,    
- Used to exchange information between databases.
- Each record in the table is one line of the text file.
- Fields are distinct in a record when separated by the tab character **`\t`**

**Tasks**
0. [data.exercise.M.1.tsv](https://github.com/cosc-526/cosc.526.home.page/blob/main/data.exercise.M.1.tsv) 
1. Repeat Task.1 using the .tsv file.  
2. The order and data in columns have changed.  
3. If you hardcoded the "age" column, research and describe alternative code to alleviate fixed positional indexes.  

**Useful links**  
- [Python docs - csv file reading and writing](https://docs.python.org/3/library/csv.html#module-csv) 
- Python handles .tsv files using its "delimiter" parameter, "\t"
"""

#=>Enter Your Solution

"""**Expected Ouput:**
```
Number of rows of data: 11
Number of cols: 3
Average Age: 62.09090909090909
```

### **`Task.3`** - Convert diacritics (ä, ö) to ASCII

- On your computer, right click an open `data.exercise.M.1.csv` in Notepad.  
Observe the Unicode non-English letters in laureates' names like "Schrödinger."
- Learn about [Unicode](https://en.wikipedia.org/wiki/Unicode) character standards for representing different types and forms of text.  
- Grok that Python 3 [natively supports](https://docs.python.org/3/howto/unicode.html) Unicode, but many tools don't.
- Conversion of Unicode to [ASCII](https://en.wikipedia.org/wiki/ASCII) formatting is often necessary in data preprocessing.  

**Tasks**
0. [data.exercise.M.1.csv](https://github.com/cosc-526/cosc.526.home.page/blob/main/data.exercise.M.1.csv)  
1. Read this article on diacritics conversion (e.g., "ü" → "ue"); [transliteration](https://german.stackexchange.com/questions/4992/conversion-table-for-diacritics-e-g-%C3%BC-%E2%86%92-ue).  
2. Analyze and run code block with a dictionary matching Unicode character "keys" to their ASCII transliteration "value."
=> as a refresher, a dictionary is defined as mydict = { key:value }
3. For labeled code sections #3.1 to 3.9, explain succinctly what the code is accomplishing and whether you are or are not familiar with it.  
4. Create your inventory mechanism to store this, and more, code blocks (ungraded & no solution).  

***More useful links***
- [1: replace](https://docs.python.org/3.6/library/stdtypes.html#str.replace), [2: file object methods](https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects),
"""

translit_dict = {
    "ä" : "ae",
    "ö" : "oe",
    "ü" : "ue",
    "Ä" : "Ae",
    "Ö" : "Oe",
    "Ü" : "Ue", 
    "ł" : "l",
    "ō" : "o",
}
#3.1
with open("data.exercise.M.1.csv", 'r', encoding='utf8') as csvfile:
    lines = csvfile.readlines()
#3.2
# Strip off the newline from the end of each line
lines = [line.rstrip() for line in lines]

#3.3   
# Split each line based on the delimiter (which, in this case, is the comma)
split_lines = [line.split(",") for line in lines]

#3.4
# Separate the header from the data
header = split_lines[0]
data_lines = split_lines[1:]
    
#3.5    
# Find "name" within the header
name_index = header.index("name")

#3.6
# Extract the names from the rows
unicode_names = [line[name_index] for line in data_lines]

#3.7
# Iterate over the names
translit_names = []
for unicode_name in unicode_names:
    # Perform the replacements in the translit_dict
    # HINT: ref [1]
    translit_name = unicode_name
    for key, value in translit_dict.items():
        translit_name = translit_name.replace(key, value)
    translit_names.append(translit_name)

#3.8
# Write out the names to a file named "data-ascii.txt"
# HINT: ref [2]
with open("data.exercise.M.1.ascii.txt", 'w') as outfile:
    for name in translit_names:
        outfile.write(name + "\n")
#3.9
# Verify that the names were converted and written out correctly
with open("data.exercise.M.1.ascii.txt", 'r') as infile:
    for line in infile:
        print(line.rstrip())

# Write your reflection here
#3.1
#3.2
#3.3
#3.4
#3.5
#3.6
#3.7
#3.8
#3.9

"""**`Expected Output`**
```
Richard Phillips Feynman
Shin'ichiro Tomonaga
Julian Schwinger
Rudolf Ludwig Moessbauer
Erwin Schroedinger
Paul Dirac
Maria Sklodowska-Curie
Pierre Curie
```

## **`assign.M.1.assignment.1`** - Data Mining with Covid Data

### **`Overview and Directions`**

1. Import and manipulate a .csv file  
2. Assess your Python Programming Skills  
3. Other assignments are more challenging; use this to assess your skills.
4. Prepare questions for a class discussion to help source additional tools. 
5. Perform tasks without assistance from clever sources.    

#### **`Desired outcomes`**  
- Experience Pandas dataframes to group, aggregate, find, sort, and calculate.  
- Perform calculations to find best country, rank, and total items processed. 
- Note: Pandas is reviewed in Module 2 and quality resources provided below.  

#### **`Additional resources`**  
- [Daniel Chen](https://github.com/chendaniely/) is a **generous** Pandas master.  
=> Purchase of his books is recommended; not a solicitation!    
- [Chen,D.,(2022). Pandas for everyone, 2nd.Ed.](https://www.amazon.com/Pandas-Everyone-Analysis-Addison-Wesley-Analytics/dp/0137891156/ref=sr_1_1?crid=T9BF3HU24YFL&keywords=pandas+for+everyone&qid=1685205022&sprefix=pandas+for+everyone%2Caps%2C203&sr=8-1)  
=> [groupby](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/07-groupby.ipynb) => [missing values](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/03-missing.ipynb) => [many more!](https://github.com/chendaniely/2017-10-26-python_crash_course/tree/gh-pages/notebooks)

### **`Task.0`**

#### **`Dataset`**
- COVID-19 variant [sequencing](https://www.cdc.gov/coronavirus/2019-ncov/variants/genomic-surveillance.html#:~:text=Scientists%20use%20a%20process%20called%20genomic%20sequencing%20to%20identify%20SARS,test%20positive%20for%20COVID%2D19) by countries.   
Data fields    
1. `location`: the country providing information.    
2. `date`: data entry date.  
3. `variant`: the COVID-19 variant for the entered record.  
4. `num_sequences`: the number of sequences **processed** by country, variant, and date.   
5. `num_sequences_total`: the number of sequences **available** by country, variant, and date.  
6. `perc_sequences`: the percentage of the available sequences processed (*out of 100*)  
`note:` each dataset row represents *one* variant by *one* country on *one* day.  

**Tasks**  
1. Locate and read dataset into a pandas.DataFrame called 'df' via  
a. A Kaggle API; use existing or acquire; [Kaggle.covid.dataset](https://www.kaggle.com/yamqwe/omicron-covid19-variant-daily-cases?select=covid-variants.csv)  
or  
b. Class github URL or another .csv method like [Matthes, Ch.16](https://github.com/cosc-526/cosc.526.home.page/blob/main/textbook.Python.crash.course.matthes.pdf)    
=> filename: **data.assignment.M.1.covid.data.csv**  
=>**consider** reading a Github data URL requires a path to **raw data**    
2. Display the DataFrame's first 5 rows.  
3. Display descriptive stats confirming: 100,416 data records.  
4. Round DataFrame to 1 decimal place!   

**Useful links**  
[Built-in Functions](https://docs.python.org/3/library/functions.html#built-in-functions)  
[pandas.DataFrame documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)
"""

## Enter solution here

"""#### **`Task.0 - Expected Outcome`**  
```
DataFrame header
  location  date        variant  num_sequences  perc_sequences  num_sequences_total
0   Angola  2020-07-06      Alpha              0             0.0   3
1   Angola  2020-07-06  B.1.1.277              0             0.0   3
2   Angola  2020-07-06  B.1.1.302              0             0.0   3
3   Angola  2020-07-06  B.1.1.519              0             0.0   3
4   Angola  2020-07-06    B.1.160              0             0.0   3

Dataframe descriptive statistics, rounded to tenths
       num_sequences  perc_sequences  num_sequences_total
count       100416.0        100416.0             100416.0
mean            72.0             6.0               1510.0
std           1669.0            22.0               8445.0
min              0.0            -0.0                  1.0
25%              0.0             0.0                 12.0
50%              0.0             0.0                 59.0
75%              0.0             0.0                394.0
max         142280.0           100.0             146170.0 
```

### **`Task.1`** - Find uncommon variants

The U.S. experienced the COVID-19 `Alpha`, `Delta`, `Omicron`  

**Tasks**  
0. In whatever object you like, e.g. list, dataframe, etc  
1. Get unique variant items for category: **`US_and_other`**  
=> where variants == [US, `non_who`, `others`]  
2. Get unique variant items for category: **`nonUS_and_other`**  
=> where variants != [US, `non_who`, `others`]  
3. Print your chosen objects to display unique variant categories.  
4. Show a total unique count for each, and total for dataset,

**Useful links**   
- [len()](https://docs.python.org/3/library/functions.html#len)
- [list comprehension w Bro Code](https://www.youtube.com/watch?v=fcLDzKH_5XM)
"""

#=>Enter Your Solution

"""#### **`Task.1 - Expected Outcome`**  
```
note: organization of output can vary widely!  

['Alpha', 'Delta', 'Omicron', 'others', 'non_who']  

total US + other =  5

['B.1.1.277', 'B.1.1.302', 'B.1.1.519', 'B.1.160', 'B.1.177', 'B.1.221',  
 'B.1.258', 'B.1.367', 'B.1.620', 'Beta', 'Epsilon', 'Eta', 'Gamma', 'Iota',  
  'Kappa', 'Lambda', 'Mu', 'S:677H.Robin1', 'S:677P.Pelican']   
  
total nonUS+other =  19   

total unique variants =  24 
```

### **`Task.2`** - Find the most processed variant

**Tasks**  
1. Which variant of COVID-19 has the most sequences processed?  
2. Store and print the result in a string called **`variant_most_proc`**  

**Useful links**  
[pd.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas-dataframe-groupby), [pd.DataFrame.aggregate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html#pandas-dataframe-aggregate)
"""

#=>Enter Your Solution

"""#### **`Task.2 - Expected Outcome`**  
```
Delta  
```

### **`Task.3`** - Find the best country at processing ALL variant sequences

**Tasks**  
1. Which country did the best processing **all** categories.    
2. Store the result in a string called **`best_proc_country`**  
3. The outcome is a single country.  
4. **consider** df.groupby("location").aggregate({"num_sequences": "sum", "num_sequences_total": "sum"})

**Useful links**  
[youtube: aggregate with groupby and .agg or .aggregate](https://www.youtube.com/watch?v=PNzlx3CjqAE)
"""

#=>Enter Your Solution

"""#### **`Task.3 - Expected Outcome`**
```
Total percent performed by country 

location                percent
Cyprus                  8.19
Hungary                 7.96
Egypt                   7.77
United Arab Emirates    7.58
Uruguay                 7.26
                        ... 
Seychelles              4.25
Fiji                    4.23
Slovakia                4.23
Brunei                  4.22
Vietnam                 4.18
Name: perc_sequences, Length: 121, dtype: float64 

the best country is =>  Cyprus
```

### **`Task.4a`** - Find the best country at processing specific variant sequences

**Tasks**  
1. Which country is best at processing sequences for Alpha, Delta, and Omicron variants?    
2. Store and print the result in a string called **`best_proc_country_ADO`** 
3. The final output is a single country.  

**Useful links** 
- ibid
"""

#=>Enter Your Solution

"""#### **`Task.4a - Expected Outcome`**  
```
best_proc_country_ADO = Vietnam  
```

### **`Task.4b`** - Find the United States ranking for processing Alpha, Delta, and Omicron

**Tasks**  
Given the outcome in 4a
1. Find the positional index value for the US ranking for processing sequences for Alpha, Delta, an Omicron variants.   
2. Store and print the ranking as an integer in a **us_ranking** variable.  
3. Ensure your ranking scale reflects a scale starting at 1.  
4. As a refresher, Python indexing starts at 0.  

**Useful links** 
- [enumerate](https://docs.python.org/3/library/functions.html#enumerate)
"""

#=>Enter Your Solution

"""#### **`Task.4b - Expected Outcome`**  
```
United States ranking = 57  
```

### **`Task.5.<final.task> - Write instructions for a jr. data scientist assignment`**
**`Task =>`**  
- Write clear and precise directions that enable your  new junior  
- data analyst, aka "Jr," to modify and fix code that you provide.

#### **`Grading requirements=>`**  
A clear and precise explanation of specific activities for production code your boss needs but you dont have time to fix.

Data science requires clear explanations of tasks, methodology, and effective communication with peers. To help the new junior analyst complete their first assignment, provide a concise and precise description including  

1. `Sample Outcome`
Deliver a comprehensive report summarizing findings and insights from data analysis. Include, as needed, desired outcome format, data objects, and visualizations.  

2. `Python Code Explanation`
Use plain language to describe specific Python code to achieve the desired outcome. Refer to pandas, Python, and other library documentation to incorporate particular language.  

3. `Consider Deprecated Functions`
The provided code is outdated and broken. Encourage problem-solving skills and leverage previous experience with similar tasks. Provide relevant links for reverse engineering.  

**`Additional personnel considerations`**
4. `Plain Language Explanation`
Consider the junior analyst's background in C and provide clear and unambiguous instructions.  

5. `Documentation Reference`
Emphasize where to consult pandas, Python, and other library documentation to discern code mechanics and clarify concepts.  

---------------

`Your manager's original request => [memo substrate]` 

`"hey! i need by lunch` the processed sequences per country on any date`  
because as CFO wants to crunch numbers this afternoon - thx Lambda"

1. Determine the percentage of processed sequences for the Alpha, Delta, and Omicron variants in the US.  
2. Store the result as a dictionary where keys are variant names and values are percentages.  
3. Save in variable = proc_seq_us

`=> Other implied items based on same exercise for manager last year`
- Determine each country's total processed sequences for Omicron on December 27, 2021 or any other date entered (date updated from 2020).
- provide country name and # processed sequences
- bidirectional sorting
- store outcomes in tuple like mytuple(country_name, processed_sequen, ) 
- variables totals like `total_omicron_2021`
"""

#=> Enter Your Solution; i.e. write memo here

"""#### `5a - code you found from last year's excercise `"""

#=> I of III - broken code last year

total_omicron_2021 = []
#5.1
df = df.set_index("location")
#5.2
df = df.loc[df6["date"] == "2021-12-27"]
#5.3
df = df6.loc[df6["variant"] == "Omicron"]
#5.3
df = df6["num_sequences"]
#5.4
 = list(zip(df.index, df))
#5.5
df7 = pd.DataFrame(sorted(missing, key=lambda x: x[1], reverse=True))
print(df7)

"""##### **`5a - Expected Outcome`**

```
0	                1
0	United Kingdom	52456
1	United States	  24681
2	Denmark	        3331
3	Germany	        1701
4	Israel	        1578
...	...	...
59	Vietnam	      1
60	Moldova	      0
61	Monaco	      0
62	Nepal	        0
63	South Korea 	0
64 rows × 2 columns
```

#### `5b - code you found from last year's excercise `
"""

#=> II of III - broken code last year

proc_seq_us = {}
df2 = df.groupby(["location", "variant"]).aggregate({
    "num_sequences": "sum",
    "num_sequences_total": "sum",
})
df2["perc_sequences"] = (df2["num_sequences"] / df2["num_sequences_total"]) * 100
df2 = df2.loc[("United States", ["Alpha", "Delta", "Omicron"]), :].loc["United States"]
df2 = df2["perc_sequences"]
proc_seq_us = df2.to_dict()
print(proc_seq_us)

"""##### **`5b - Expected Outcome`**

```
{'Alpha': 11.520951617373877, 'Delta': 63.76796208057254, 
                                                'Omicron': 1.370817855027461}
```

#### `5c - code you found from last year's excercise `
"""

#=> III of III - broken code last year

total_omicron_2021 = []
#5.1
df6 = df.set_index("location")
#5.2
df6 = df6.loc[df6["date"] == "2021-12-27"]
#5.3#
df6 = df6.loc[df6["variant"] == "Omicron"]
#5.3
df6 = df6["num_sequences"]
#5.4
total_omicron_2021 = list(zip(df6.index, df6))
#5.5
df7 = pd.DataFrame(sorted(total_omicron_2021, key=lambda x: x[1], reverse=True))
print(df7)
total_omicron_2021 = sorted(total_omicron_2021, key=lambda x: x[1], reverse=True)
print(total_omicron_2021)

"""##### **`5c - Expected Outcome`**

```
                 0      1
0   United Kingdom  52456
1    United States  24681
2          Denmark   3331
3          Germany   1701
4           Israel   1578
..             ...    ...
59         Vietnam      1
60         Moldova      0
61          Monaco      0
62           Nepal      0
63     South Korea      0
[64 rows x 2 columns]

#[('United Kingdom', 52456), ('United States', 24681), ('Denmark', 3331),
 ('Germany', 1701), ('Israel', 1578), ('Australia', 1319), ('Switzerland', 514),
  ('France', 509), ('Italy', 486), ('Belgium', 464), ('Spain', 461), 
  ('Sweden', 434), ('Chile', 260), ('Netherlands', 254), ('Singapore', 249),
  ('Mexico', 240), ('Turkey', 202), ('India', 174), ('Brazil', 147),
   ('Botswana', 142), ('Indonesia', 128), ('Japan', 118), ('Portugal', 118),
    ('Argentina', 80), ('New Zealand', 63), ('South Africa', 61), 
    ('Lithuania', 50), ('Czechia', 49), ('Georgia', 46), ('Russia', 45), 
    ('Colombia', 37), ('Sri Lanka', 37), ('Hong Kong', 35), ('Malta', 34),
     ('Poland', 28), ('Ecuador', 26), ('Canada', 25), ('Jordan', 22), 
     ('Malawi', 21), ('Cambodia', 18), ('Norway', 17), ('Morocco', 15), 
     ('Senegal', 15), ('Costa Rica', 14), ('Pakistan', 11), ('Nigeria', 10),
      ('Peru', 10), ('Brunei', 8), ('Slovakia', 8), ('Trinidad and Tobago', 8),
       ('Maldives', 7), ('Zambia', 7), ('Thailand', 6), ('Malaysia', 5), 
       ('Bangladesh', 4), ('Romania', 3), ('Iran', 1), ('Oman', 1),
        ('Ukraine', 1), ('Vietnam', 1), ('Moldova', 0), ('Monaco', 0), 
        ('Nepal', 0), ('South Korea', 0)]
```

### `assignment.M1.Housekeeping`
"""

#from google.colab import drive
#drive.flush_and_unmount()

##=>Perform A,B,C when done with spark
#A: 
#=> Close the SparkSession
spark.stop()

#B:
#=> Disconnect and stop Spark in a Jupyter Notebook,
#=> stops SparkContext and releases its resourcs
sc.stop()

#C: 
#=> Confirm Spark termination by checking the Spark UI
#=> Access UI by visiting URL provided in Notebook output where Spark fireup

"""# `<end.M1` ~ `M1.end>`
-------------
"""